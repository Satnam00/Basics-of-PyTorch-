{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport torch","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input (temp, rainfall, humidity)\ninputs = np.array([[73, 67, 43], \n                   [91, 88, 64], \n                   [87, 134, 58], \n                   [102, 43, 37], \n                   [69, 96, 70]], dtype='float32')\n# Targets (apples, oranges)\ntargets = np.array([[56, 70], \n                    [81, 101], \n                    [119, 133], \n                    [22, 37], \n                    [103, 119]], dtype='float32')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs.shape","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"(5, 3)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets.shape","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"(5, 2)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert inputs and targets to tensors\ninputs = torch.from_numpy(inputs)\ntargets = torch.from_numpy(targets)\nprint(inputs)\nprint(targets)","execution_count":5,"outputs":[{"output_type":"stream","text":"tensor([[ 73.,  67.,  43.],\n        [ 91.,  88.,  64.],\n        [ 87., 134.,  58.],\n        [102.,  43.,  37.],\n        [ 69.,  96.,  70.]])\ntensor([[ 56.,  70.],\n        [ 81., 101.],\n        [119., 133.],\n        [ 22.,  37.],\n        [103., 119.]])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Weights and biases\nw = torch.randn(2, 3, requires_grad=True)\nb = torch.randn(2, requires_grad=True)\nprint(w)\nprint(b)","execution_count":6,"outputs":[{"output_type":"stream","text":"tensor([[-2.7338,  1.1521,  0.4068],\n        [ 0.8389, -0.6128, -2.3575]], requires_grad=True)\ntensor([ 1.0640, -0.5233], requires_grad=True)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model(x):\n    return x @ w.t() + b","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate predictions\npreds = model(inputs)\nprint(preds)","execution_count":8,"outputs":[{"output_type":"stream","text":"tensor([[-103.8177,  -81.7122],\n        [-120.2882, -128.9881],\n        [ -58.7990, -146.3894],\n        [-213.1877,  -28.5298],\n        [ -48.4877, -166.4925]], grad_fn=<AddBackward0>)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare with targets\nprint(targets)","execution_count":9,"outputs":[{"output_type":"stream","text":"tensor([[ 56.,  70.],\n        [ 81., 101.],\n        [119., 133.],\n        [ 22.,  37.],\n        [103., 119.]])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MSE loss\ndef mse(t1, t2):\n    diff = t1 - t2\n    return torch.sum(diff * diff) / diff.numel()","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute loss\nloss = mse(preds, targets)\nprint(loss)","execution_count":11,"outputs":[{"output_type":"stream","text":"tensor(41570.2578, grad_fn=<DivBackward0>)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute gradients\nloss.backward()","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gradients for weights\nprint(w)\nprint(w.grad)","execution_count":13,"outputs":[{"output_type":"stream","text":"tensor([[-2.7338,  1.1521,  0.4068],\n        [ 0.8389, -0.6128, -2.3575]], requires_grad=True)\ntensor([[-15978.8477, -15380.4238,  -9874.6074],\n        [-16538.7598, -19613.3809, -11971.3047]])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"w.grad.zero_()\nb.grad.zero_()\nprint(w.grad)\nprint(b.grad)","execution_count":14,"outputs":[{"output_type":"stream","text":"tensor([[0., 0., 0.],\n        [0., 0., 0.]])\ntensor([0., 0.])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate predictions\npreds = model(inputs)\nprint(preds)","execution_count":15,"outputs":[{"output_type":"stream","text":"tensor([[-103.8177,  -81.7122],\n        [-120.2882, -128.9881],\n        [ -58.7990, -146.3894],\n        [-213.1877,  -28.5298],\n        [ -48.4877, -166.4925]], grad_fn=<AddBackward0>)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the loss\nloss = mse(preds, targets)\nprint(loss)","execution_count":16,"outputs":[{"output_type":"stream","text":"tensor(41570.2578, grad_fn=<DivBackward0>)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute gradients\nloss.backward()\nprint(w.grad)\nprint(b.grad)","execution_count":17,"outputs":[{"output_type":"stream","text":"tensor([[-15978.8477, -15380.4238,  -9874.6074],\n        [-16538.7598, -19613.3809, -11971.3047]])\ntensor([-185.1161, -202.4224])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adjust weights & reset gradients\nwith torch.no_grad():\n    w -= w.grad * 1e-5\n    b -= b.grad * 1e-5\n    w.grad.zero_()\n    b.grad.zero_()","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(w)\nprint(b)","execution_count":19,"outputs":[{"output_type":"stream","text":"tensor([[-2.5740,  1.3059,  0.5056],\n        [ 1.0043, -0.4167, -2.2378]], requires_grad=True)\ntensor([ 1.0659, -0.5212], requires_grad=True)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate loss\npreds = model(inputs)\nloss = mse(preds, targets)\nprint(loss)","execution_count":20,"outputs":[{"output_type":"stream","text":"tensor(28903.1680, grad_fn=<DivBackward0>)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train for 100 epochs\nfor i in range(2000):\n    preds = model(inputs)\n    loss = mse(preds, targets)\n    loss.backward()\n    with torch.no_grad():\n        w -= w.grad * 1e-5\n        b -= b.grad * 1e-5\n        w.grad.zero_()\n        b.grad.zero_()","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"tensor([[-0.4136,  0.8400,  0.7013],\n        [-0.2072,  0.9442,  0.5335]], requires_grad=True)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"b","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"tensor([ 1.0834, -0.5130], requires_grad=True)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate loss\npreds = model(inputs)\nloss = mse(preds, targets)\nprint(loss)","execution_count":24,"outputs":[{"output_type":"stream","text":"tensor(8.7398, grad_fn=<DivBackward0>)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predictions\npreds","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"tensor([[ 57.3277,  70.5637],\n        [ 82.2501,  97.8653],\n        [118.3369, 138.9266],\n        [ 20.9669,  38.6943],\n        [102.2759, 113.1774]], grad_fn=<AddBackward0>)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Targets\ntargets","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"tensor([[ 56.,  70.],\n        [ 81., 101.],\n        [119., 133.],\n        [ 22.,  37.],\n        [103., 119.]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train for 100 epochs\nfor i in range(4500):\n    preds = model(inputs)\n    loss = mse(preds, targets)\n    loss.backward()\n    with torch.no_grad():\n        w -= w.grad * 1e-5\n        b -= b.grad * 1e-5\n        w.grad.zero_()\n        b.grad.zero_()","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate loss\npreds = model(inputs)\nloss = mse(preds, targets)\nprint(loss)","execution_count":28,"outputs":[{"output_type":"stream","text":"tensor(0.5272, grad_fn=<DivBackward0>)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predictions\npreds","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"tensor([[ 57.3460,  70.2284],\n        [ 82.0959, 100.6814],\n        [118.6582, 133.0611],\n        [ 21.0572,  37.0509],\n        [101.9549, 119.0356]], grad_fn=<AddBackward0>)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Targets\ntargets","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"tensor([[ 56.,  70.],\n        [ 81., 101.],\n        [119., 133.],\n        [ 22.,  37.],\n        [103., 119.]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install jovian --upgrade -q","execution_count":31,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear regression using PyTorch built-ins\n\nThe model and training process above were implemented using basic matrix operations. But since this such a common pattern , PyTorch has several built-in functions and classes to make it easy to create and train models.\n\nLet's begin by importing the `torch.nn` package from PyTorch, which contains utility classes for building neural networks."},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input (temp, rainfall, humidity)\ninputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n                  dtype='float32')\n\n# Targets (apples, oranges)\ntargets = np.array([[56, 70], [81, 101], [119, 133], \n                    [22, 37], [103, 119], [56, 70], \n                    [81, 101], [119, 133], [22, 37], \n                    [103, 119], [56, 70], [81, 101], \n                    [119, 133], [22, 37], [103, 119]], \n                   dtype='float32')\n\ninputs = torch.from_numpy(inputs)\ntargets = torch.from_numpy(targets)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"tensor([[ 73.,  67.,  43.],\n        [ 91.,  88.,  64.],\n        [ 87., 134.,  58.],\n        [102.,  43.,  37.],\n        [ 69.,  96.,  70.],\n        [ 73.,  67.,  43.],\n        [ 91.,  88.,  64.],\n        [ 87., 134.,  58.],\n        [102.,  43.,  37.],\n        [ 69.,  96.,  70.],\n        [ 73.,  67.,  43.],\n        [ 91.,  88.,  64.],\n        [ 87., 134.,  58.],\n        [102.,  43.,  37.],\n        [ 69.,  96.,  70.]])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Dataset and DataLoader\n\nWe'll create a `TensorDataset`, which allows access to rows from `inputs` and `targets` as tuples, and provides standard APIs for working with many different types of datasets in PyTorch."},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import TensorDataset","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define dataset\ntrain_ds = TensorDataset(inputs, targets)\ntrain_ds[0:3]","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"(tensor([[ 73.,  67.,  43.],\n         [ 91.,  88.,  64.],\n         [ 87., 134.,  58.]]),\n tensor([[ 56.,  70.],\n         [ 81., 101.],\n         [119., 133.]]))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define data loader\nbatch_size = 5\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True)","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for xb, yb in train_dl:\n    print(xb)\n    print(yb)\n    break","execution_count":39,"outputs":[{"output_type":"stream","text":"tensor([[102.,  43.,  37.],\n        [ 87., 134.,  58.],\n        [ 69.,  96.,  70.],\n        [ 87., 134.,  58.],\n        [ 91.,  88.,  64.]])\ntensor([[ 22.,  37.],\n        [119., 133.],\n        [103., 119.],\n        [119., 133.],\n        [ 81., 101.]])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## nn.Linear\n\nInstead of initializing the weights & biases manually, we can define the model using the `nn.Linear` class from PyTorch, which does it automatically."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define model\nmodel = nn.Linear(3, 2)\nprint(model.weight)\nprint(model.bias)","execution_count":40,"outputs":[{"output_type":"stream","text":"Parameter containing:\ntensor([[-0.4507,  0.2257, -0.1988],\n        [-0.3660, -0.4758,  0.3986]], requires_grad=True)\nParameter containing:\ntensor([-0.3918,  0.0508], requires_grad=True)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters\nlist(model.parameters())","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"[Parameter containing:\n tensor([[-0.4507,  0.2257, -0.1988],\n         [-0.3660, -0.4758,  0.3986]], requires_grad=True),\n Parameter containing:\n tensor([-0.3918,  0.0508], requires_grad=True)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We can use the model to generate predictions in the exact same way as before:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate predictions\npreds = model(inputs)\npreds","execution_count":42,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"tensor([[-26.7165, -41.4072],\n        [-34.2634, -49.6169],\n        [-20.8838, -72.4300],\n        [-44.0112, -42.9943],\n        [-23.7353, -42.9794],\n        [-26.7165, -41.4072],\n        [-34.2634, -49.6169],\n        [-20.8838, -72.4300],\n        [-44.0112, -42.9943],\n        [-23.7353, -42.9794],\n        [-26.7165, -41.4072],\n        [-34.2634, -49.6169],\n        [-20.8838, -72.4300],\n        [-44.0112, -42.9943],\n        [-23.7353, -42.9794]], grad_fn=<AddmmBackward>)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Loss Function\n\nInstead of defining a loss function manually, we can use the built-in loss function `mse_loss`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import nn.functional\nimport torch.nn.functional as F","execution_count":43,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `nn.functional` package contains many useful loss functions and several other utilities. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define loss function\nloss_fn = F.mse_loss","execution_count":44,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's compute the loss for the current predictions of our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = loss_fn(model(inputs), targets)\nprint(loss)","execution_count":45,"outputs":[{"output_type":"stream","text":"tensor(17004.9395, grad_fn=<MseLossBackward>)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Optimizer\n\nInstead of manually manipulating the model's weights & biases using gradients, we can use the optimizer `optim.SGD`. SGD stands for `stochastic gradient descent`. It is called `stochastic` because samples are selected in batches (often with random shuffling) instead of as a single group."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define optimizer\nopt = torch.optim.SGD(model.parameters(), lr=1e-5)","execution_count":46,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that `model.parameters()` is passed as an argument to `optim.SGD`, so that the optimizer knows which matrices should be modified during the update step. Also, we can specify a learning rate which controls the amount by which the parameters are modified."},{"metadata":{},"cell_type":"markdown","source":"## Train the model\n\nWe are now ready to train the model. We'll follow the exact same process to implement gradient descent:\n\n1. Generate predictions\n\n2. Calculate the loss\n\n3. Compute gradients w.r.t the weights and biases\n\n4. Adjust the weights by subtracting a small quantity proportional to the gradient\n\n5. Reset the gradients to zero\n\nThe only change is that we'll work batches of data, instead of processing the entire training data in every iteration. Let's define a utility function `fit` which trains the model for a given number of epochs."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Utility function to train the model\ndef fit(num_epochs, model, loss_fn, opt, train_dl):\n    \n    # Repeat for given number of epochs\n    for epoch in range(num_epochs):\n        \n        # Train with batches of data\n        for xb,yb in train_dl:\n            \n            # 1. Generate predictions\n            pred = model(xb)\n            \n            # 2. Calculate loss\n            loss = loss_fn(pred, yb)\n            \n            # 3. Compute gradients\n            loss.backward()\n            \n            # 4. Update parameters using gradients\n            opt.step()\n            \n            # 5. Reset the gradients to zero\n            opt.zero_grad()\n        \n        # Print the progress\n        if (epoch+1) % 10 == 0:\n            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit(100, model, loss_fn, opt, train_dl)","execution_count":48,"outputs":[{"output_type":"stream","text":"Epoch [10/100], Loss: 127.8820\nEpoch [20/100], Loss: 87.3328\nEpoch [30/100], Loss: 68.6895\nEpoch [40/100], Loss: 106.0618\nEpoch [50/100], Loss: 47.3130\nEpoch [60/100], Loss: 50.4639\nEpoch [70/100], Loss: 26.2628\nEpoch [80/100], Loss: 11.9688\nEpoch [90/100], Loss: 16.2707\nEpoch [100/100], Loss: 8.4678\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Some things to note above:\n\n* We use the data loader defined earlier to get batches of data for every iteration.\n\n* Instead of updating parameters (weights and biases) manually, we use `opt.step` to perform the update, and `opt.zero_grad` to reset the gradients to zero.\n\n* We've also added a log statement which prints the loss from the last batch of data for every 10th epoch, to track the progress of training. `loss.item` returns the actual value stored in the loss tensor.\n\nLet's train the model for 100 epochs."},{"metadata":{"trusted":true},"cell_type":"code","source":"fit(1000, model, loss_fn, opt, train_dl)","execution_count":49,"outputs":[{"output_type":"stream","text":"Epoch [10/1000], Loss: 15.7812\nEpoch [20/1000], Loss: 5.3975\nEpoch [30/1000], Loss: 21.1533\nEpoch [40/1000], Loss: 13.3486\nEpoch [50/1000], Loss: 8.5648\nEpoch [60/1000], Loss: 1.3637\nEpoch [70/1000], Loss: 10.8330\nEpoch [80/1000], Loss: 6.3020\nEpoch [90/1000], Loss: 4.5924\nEpoch [100/1000], Loss: 7.3259\nEpoch [110/1000], Loss: 6.9167\nEpoch [120/1000], Loss: 12.6976\nEpoch [130/1000], Loss: 4.7627\nEpoch [140/1000], Loss: 4.4613\nEpoch [150/1000], Loss: 6.2067\nEpoch [160/1000], Loss: 3.9803\nEpoch [170/1000], Loss: 5.8236\nEpoch [180/1000], Loss: 5.6693\nEpoch [190/1000], Loss: 2.7416\nEpoch [200/1000], Loss: 4.2112\nEpoch [210/1000], Loss: 3.1465\nEpoch [220/1000], Loss: 3.0218\nEpoch [230/1000], Loss: 1.1256\nEpoch [240/1000], Loss: 2.1762\nEpoch [250/1000], Loss: 2.1117\nEpoch [260/1000], Loss: 1.1763\nEpoch [270/1000], Loss: 2.5358\nEpoch [280/1000], Loss: 2.2734\nEpoch [290/1000], Loss: 1.9285\nEpoch [300/1000], Loss: 3.5903\nEpoch [310/1000], Loss: 1.8371\nEpoch [320/1000], Loss: 1.9195\nEpoch [330/1000], Loss: 2.9573\nEpoch [340/1000], Loss: 1.4266\nEpoch [350/1000], Loss: 0.8421\nEpoch [360/1000], Loss: 1.5269\nEpoch [370/1000], Loss: 0.8891\nEpoch [380/1000], Loss: 1.8499\nEpoch [390/1000], Loss: 1.2943\nEpoch [400/1000], Loss: 1.1658\nEpoch [410/1000], Loss: 1.2337\nEpoch [420/1000], Loss: 1.1893\nEpoch [430/1000], Loss: 0.4723\nEpoch [440/1000], Loss: 1.1783\nEpoch [450/1000], Loss: 1.8280\nEpoch [460/1000], Loss: 1.0275\nEpoch [470/1000], Loss: 0.5133\nEpoch [480/1000], Loss: 1.2179\nEpoch [490/1000], Loss: 0.9428\nEpoch [500/1000], Loss: 1.0058\nEpoch [510/1000], Loss: 1.0138\nEpoch [520/1000], Loss: 1.4816\nEpoch [530/1000], Loss: 1.0354\nEpoch [540/1000], Loss: 0.4718\nEpoch [550/1000], Loss: 0.8510\nEpoch [560/1000], Loss: 0.3016\nEpoch [570/1000], Loss: 0.2889\nEpoch [580/1000], Loss: 0.8317\nEpoch [590/1000], Loss: 0.8885\nEpoch [600/1000], Loss: 0.7158\nEpoch [610/1000], Loss: 0.7852\nEpoch [620/1000], Loss: 0.3645\nEpoch [630/1000], Loss: 0.7476\nEpoch [640/1000], Loss: 0.3647\nEpoch [650/1000], Loss: 0.8046\nEpoch [660/1000], Loss: 1.2023\nEpoch [670/1000], Loss: 0.7716\nEpoch [680/1000], Loss: 0.8000\nEpoch [690/1000], Loss: 0.5696\nEpoch [700/1000], Loss: 0.6713\nEpoch [710/1000], Loss: 0.6795\nEpoch [720/1000], Loss: 0.6004\nEpoch [730/1000], Loss: 0.5296\nEpoch [740/1000], Loss: 0.9558\nEpoch [750/1000], Loss: 0.4068\nEpoch [760/1000], Loss: 0.7508\nEpoch [770/1000], Loss: 0.7473\nEpoch [780/1000], Loss: 0.4910\nEpoch [790/1000], Loss: 0.5044\nEpoch [800/1000], Loss: 0.6987\nEpoch [810/1000], Loss: 0.5401\nEpoch [820/1000], Loss: 0.9126\nEpoch [830/1000], Loss: 0.3357\nEpoch [840/1000], Loss: 0.4742\nEpoch [850/1000], Loss: 0.5798\nEpoch [860/1000], Loss: 0.8012\nEpoch [870/1000], Loss: 0.9135\nEpoch [880/1000], Loss: 0.7947\nEpoch [890/1000], Loss: 0.4137\nEpoch [900/1000], Loss: 0.4989\nEpoch [910/1000], Loss: 0.7012\nEpoch [920/1000], Loss: 0.5387\nEpoch [930/1000], Loss: 0.6327\nEpoch [940/1000], Loss: 0.6150\nEpoch [950/1000], Loss: 0.4709\nEpoch [960/1000], Loss: 0.7455\nEpoch [970/1000], Loss: 0.3480\nEpoch [980/1000], Loss: 0.6829\nEpoch [990/1000], Loss: 0.4930\nEpoch [1000/1000], Loss: 0.8592\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Let's generate predictions using our model and verify that they're close to our targets."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate predictions\npreds = model(inputs)\npreds","execution_count":50,"outputs":[{"output_type":"execute_result","execution_count":50,"data":{"text/plain":"tensor([[ 57.1684,  70.3139],\n        [ 82.1393, 100.6602],\n        [119.1349, 132.9487],\n        [ 21.2357,  37.0097],\n        [101.6152, 119.1361],\n        [ 57.1684,  70.3139],\n        [ 82.1393, 100.6602],\n        [119.1349, 132.9487],\n        [ 21.2357,  37.0097],\n        [101.6152, 119.1361],\n        [ 57.1684,  70.3139],\n        [ 82.1393, 100.6602],\n        [119.1349, 132.9487],\n        [ 21.2357,  37.0097],\n        [101.6152, 119.1361]], grad_fn=<AddmmBackward>)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare with targets\ntargets","execution_count":51,"outputs":[{"output_type":"execute_result","execution_count":51,"data":{"text/plain":"tensor([[ 56.,  70.],\n        [ 81., 101.],\n        [119., 133.],\n        [ 22.,  37.],\n        [103., 119.],\n        [ 56.,  70.],\n        [ 81., 101.],\n        [119., 133.],\n        [ 22.,  37.],\n        [103., 119.],\n        [ 56.,  70.],\n        [ 81., 101.],\n        [119., 133.],\n        [ 22.,  37.],\n        [103., 119.]])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Indeed, the predictions are quite close to our targets, and now we have a fairly good model to predict crop yields for apples and oranges by looking at the average temperature, rainfall and humidity in a region."},{"metadata":{},"cell_type":"markdown","source":"## Commit and update the notebook\n\nAs a final step, we can record a new version of the notebook using the `jovian` library."},{"metadata":{"trusted":true},"cell_type":"code","source":"import jovian","execution_count":52,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"if (window.IPython && IPython.notebook.kernel) IPython.notebook.kernel.execute('jovian.utils.jupyter.get_notebook_name_saved = lambda: \"' + IPython.notebook.notebook_name + '\"')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"jovian.commit(project='linear-regression-pytorch')","execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"},"metadata":{}},{"output_type":"stream","text":"[jovian] Attempting to save notebook..\u001b[0m\n[jovian] Detected Kaggle notebook...\u001b[0m\n[jovian] Please enter your API key ( from https://jovian.ml/ ):\u001b[0m\nAPI KEY: ········\n[jovian] Uploading notebook to https://jovian.ml/satnam00/linear-regression-pytorch\u001b[0m\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n    require([\"base/js/namespace\"],function(Jupyter) {\n        var nbJson = JSON.stringify(Jupyter.notebook.toJSON());\n\n        console.log(\"[jovian] Extracted notebook JSON:\");\n        console.log(nbJson);\n\n        function jvnLog (data) {\n          console.log(\"Result from jovian.commit:\");\n          if (data.content.text) {\n              var result = JSON.parse(data.content.text.trim());\n              var msg = result['msg'];\n              var err = result['err'];\n              if (msg) {\n                  element.text(\"Committed successfully: \" + msg)\n              } else {\n                  alert(\"Notebook commit failed. Error: \" + (err || \"Unknown\"))\n              }\n          }\n          \n        };\n        \n        var pythonCode = `\nfrom contextlib import redirect_stdout, redirect_stderr\nfrom io import StringIO\nimport json\n \nwith open(\"linear-regression-pytorch.ipynb\", 'w') as f:\n    f.write(r\"\"\"${nbJson}\"\"\")\n\njvn_update = StringIO()\njvn_update_err = StringIO()\nwith redirect_stdout(jvn_update), redirect_stderr(jvn_update_err):\n    from jovian import commit\n\njvn_f_out = StringIO()\njvn_f_err = StringIO()\nwith redirect_stdout(jvn_f_out), redirect_stderr(jvn_f_err):\n    jvn_msg = jovian.commit(message=None, files=[], outputs=[], environment='auto', privacy='auto', filename='linear-regression-pytorch.ipynb', project='linear-regression-pytorch', new_project=None)\n\nprint(json.dumps({'msg': jvn_msg, 'err': jvn_f_err.getvalue(), 'update': jvn_update.getvalue()}))\n        `;\n\n        console.log(\"Invoking jovian.commit\")\n        // console.log(pythonCode)\n\n        Jupyter.notebook.kernel.execute(pythonCode, { iopub: { output: jvnLog }});\n    });"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}